# chatbot.py
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import OllamaLLM
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.documents import Document
import re
import logging

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∏ –ø–æ–∏—Å–∫–∞
llm = OllamaLLM(model="qwen2:0.5b", temperature=0.2)
search = DuckDuckGoSearchRun()

PROMPT_TEMPLATE = """
–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ‚Äî —Å–∫–∞–∂–∏: "–í –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö –ª–µ–∫—Ü–∏–π —ç—Ç–æ–≥–æ –Ω–µ—Ç."

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}
"""

prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)

def format_docs(docs):
    return "\n\n---\n\n".join([d.page_content for d in docs])

def create_qa_chain(vectorstore):
    """–°–æ–∑–¥–∞—ë—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é RAG-—Ü–µ–ø–æ—á–∫—É"""
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return chain, retriever

def clean_text(text):
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤"""
    # –£–¥–∞–ª—è–µ–º –∫–∏—Ç–∞–π—Å–∫–∏–µ, —è–ø–æ–Ω—Å–∫–∏–µ, –∫–æ—Ä–µ–π—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –¥—Ä—É–≥–∏–µ –Ω–µ-–∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ/–ª–∞—Ç–∏–Ω—Å–∫–∏–µ
    cleaned = re.sub(r'[^\x00-\x7F\u0400-\u04FF\u0500-\u052F\s\.\,\!\?\-\:\;\(\)\d]', '', text)
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    return cleaned

def is_relevant_document(doc_content, question):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –≤–æ–ø—Ä–æ—Å—É"""
    doc_lower = doc_content.lower()
    question_lower = question.lower()
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    keywords = re.findall(r'\b\w+\b', question_lower)
    
    # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    stop_words = {'—á—Ç–æ', '—Ç–∞–∫–æ–µ', '–∫–∞–∫', '–¥–ª—è', '–Ω–∞', '–≤', '—Å', '–ø–æ', '–æ', '–æ–±', '–∏–∑', '–æ—Ç', '–¥–æ', '–∑–∞', '–∏', '–∏–ª–∏', '–Ω–æ', '–Ω–µ', '–Ω–µ—Ç', '–¥–∞', '–µ—Å—Ç—å', '–±—ã—Ç—å', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–æ–µ', '–∫–∞–∫–∏–µ'}
    keywords = [kw for kw in keywords if kw not in stop_words and len(kw) > 2]
    
    # –ï—Å–ª–∏ –Ω–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, —Å—á–∏—Ç–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –ù–ï—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º
    if not keywords:
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
    keyword_matches = sum(1 for keyword in keywords if keyword in doc_lower)
    
    # –î–æ–∫—É–º–µ–Ω—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –µ—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã 50% —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    # –ò –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
    min_required_matches = max(1, len(keywords) // 2)
    return keyword_matches >= min_required_matches and len(doc_content.strip()) > 20

def hybrid_answer(question, vectorstore):
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –æ—Ç–≤–µ—Ç: —Å–Ω–∞—á–∞–ª–∞ –ª–µ–∫—Ü–∏–∏, –ø–æ—Ç–æ–º –∏–Ω—Ç–µ—Ä–Ω–µ—Ç"""
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    docs = retriever.get_relevant_documents(question)

    print(f"üîç –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docs)}")
    for i, doc in enumerate(docs):
        print(f"–î–æ–∫—É–º–µ–Ω—Ç {i+1}: {doc.page_content[:100]}...")
        print(f"–†–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω: {is_relevant_document(doc.page_content, question)}")
        print("---")

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    relevant_docs = []
    if docs:
        for doc in docs:
            if is_relevant_document(doc.page_content, question):
                relevant_docs.append(doc)

    print(f"‚úÖ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(relevant_docs)}")

    # –ï—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–∫—Ü–∏–∏
    if relevant_docs:
        context = format_docs(relevant_docs)
        formatted_prompt = prompt.format(context=context, question=question)
        answer = llm.invoke(formatted_prompt)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏ –æ—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
        if "–≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö –ª–µ–∫—Ü–∏–π —ç—Ç–æ–≥–æ –Ω–µ—Ç" in answer.lower() or "–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏" in answer.lower():
            print("üîÑ LLM —Å–∫–∞–∑–∞–ª, —á—Ç–æ –≤ –ª–µ–∫—Ü–∏—è—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç")
            return get_internet_answer(question), [], True
        else:
            return answer, relevant_docs, False
    else:
        # –ò—â–µ–º –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
        print("üîÑ –í –ª–µ–∫—Ü–∏—è—Ö –Ω–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∏—â–µ–º –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ")
        return get_internet_answer(question), [], True

def get_internet_answer(question):
    """–ü–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞"""
    try:
        web_results = search.run(question)
        # –û—á–∏—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        cleaned_web_results = clean_text(web_results)
        
        # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞, –ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑
        if len(cleaned_web_results) < 30:
            web_results = search.run(question + " –Ω–∞ —Ä—É—Å—Å–∫–æ–º")
            cleaned_web_results = clean_text(web_results)
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–æ—Ç–≤–µ—Ç–æ–≤
        internet_prompt = f"""
        –¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞.
        –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

        –í–æ–ø—Ä–æ—Å: {question}

        –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞:
        {cleaned_web_results}

        –¢–≤–æ–π –æ—Ç–≤–µ—Ç:
        """
        
        answer = llm.invoke(internet_prompt)
        # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        answer = clean_text(answer)
        return answer
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: {e}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∏ –≤ –ª–µ–∫—Ü–∏—è—Ö, –Ω–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ."
    
def evaluate_answer_quality(question, answer, docs, used_web):
    """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞"""
    
    quality_metrics = {
        "relevance_score": 0.0,
        "completeness_score": 0.0,
        "confidence_score": 0.0,
        "overall_score": 0.0,
        "overall_quality": "unknown"
    }
    
    try:
        # 1. –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        relevance_score = calculate_relevance_score(question, answer)
        
        # 2. –û—Ü–µ–Ω–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã
        completeness_score = calculate_completeness_score(question, answer)
        
        # 3. –û—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤)
        confidence_score = calculate_confidence_score(docs, used_web)
        
        # –û–±—â–∏–π –±–∞–ª–ª
        overall_score = (relevance_score * 0.4 + 
                        completeness_score * 0.3 + 
                        confidence_score * 0.3)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞
        if overall_score >= 0.8:
            quality = "excellent"
        elif overall_score >= 0.6:
            quality = "good"
        elif overall_score >= 0.4:
            quality = "fair"
        else:
            quality = "poor"
            
        quality_metrics.update({
            "relevance_score": round(relevance_score, 2),
            "completeness_score": round(completeness_score, 2),
            "confidence_score": round(confidence_score, 2),
            "overall_score": round(overall_score, 2),
            "overall_quality": quality
        })
        
    except Exception as e:
        logging.error(f"Error in quality evaluation: {e}")
    
    return quality_metrics

def calculate_relevance_score(question, answer):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –≤–æ–ø—Ä–æ—Å—É"""
    question_lower = question.lower()
    answer_lower = answer.lower()
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    question_words = set(re.findall(r'\b\w+\b', question_lower))
    answer_words = set(re.findall(r'\b\w+\b', answer_lower))
    
    # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    stop_words = {'—á—Ç–æ', '–∫–∞–∫', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–∫—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫–∞–∫–æ–π'}
    question_words = question_words - stop_words
    answer_words = answer_words - stop_words
    
    if not question_words:
        return 0.0
    
    # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    common_words = question_words.intersection(answer_words)
    word_similarity = len(common_words) / len(question_words)
    
    # –®—Ç—Ä–∞—Ñ –∑–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    if "–≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö –ª–µ–∫—Ü–∏–π —ç—Ç–æ–≥–æ –Ω–µ—Ç" in answer_lower:
        word_similarity *= 0.1
    
    return min(word_similarity, 1.0)

def calculate_completeness_score(question, answer):
    """–û—Ü–µ–Ω–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –æ—Ç–≤–µ—Ç–∞"""
    answer_lower = answer.lower()
    
    # –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ–ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
    negative_indicators = [
        "–Ω–µ –∑–Ω–∞—é", "–Ω–µ —É–≤–µ—Ä–µ–Ω", "–Ω–µ –º–æ–≥—É –Ω–∞–π—Ç–∏", 
        "–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "–º–∞–ª–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
    ]
    
    # –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
    positive_indicators = [
        "–≤–æ-–ø–µ—Ä–≤—ã—Ö", "–≤–æ-–≤—Ç–æ—Ä—ã—Ö", "—Å –æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω—ã", 
        "—Å –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã", "–Ω–∞–ø—Ä–∏–º–µ—Ä", "—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º"
    ]
    
    completeness = 0.5  # –ë–∞–∑–æ–≤—ã–π –±–∞–ª–ª
    
    # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    for indicator in negative_indicators:
        if indicator in answer_lower:
            completeness -= 0.2
    
    # –ë–æ–Ω—É—Å—ã –∑–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    for indicator in positive_indicators:
        if indicator in answer_lower:
            completeness += 0.1
    
    # –£—á–∏—Ç—ã–≤–∞–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞ (–Ω–æ –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ)
    answer_length = len(answer.split())
    if 20 <= answer_length <= 300:
        completeness += 0.1
    elif answer_length < 10:
        completeness -= 0.2
    
    return max(0.0, min(completeness, 1.0))

def calculate_confidence_score(docs, used_web):
    """–û—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –æ—Ç–≤–µ—Ç–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    if used_web:
        return 0.6  # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    
    if not docs:
        return 0.3  # –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –±–µ–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    
    # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Ä–∞—Å—Ç–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    doc_count = len(docs)
    if doc_count >= 3:
        return 0.9
    elif doc_count == 2:
        return 0.7
    else:
        return 0.5
    
def update_answer_quality_with_feedback(question, user_feedback, previous_metrics):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–¥–±—ç–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    
    try:
        # –ë–∞–∑–æ–≤–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–∞–π–∫–∞/–¥–∏–∑–ª–∞–π–∫–∞
        feedback_multiplier = 1.2 if user_feedback == "like" else 0.8
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â–∏–π –±–∞–ª–ª
        updated_overall_score = min(1.0, max(0.0, previous_metrics["overall_score"] * feedback_multiplier))
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∫–∞—á–µ—Å—Ç–≤–∞
        if updated_overall_score >= 0.8:
            updated_quality = "excellent"
        elif updated_overall_score >= 0.6:
            updated_quality = "good"
        elif updated_overall_score >= 0.4:
            updated_quality = "fair"
        else:
            updated_quality = "poor"
        
        updated_metrics = previous_metrics.copy()
        updated_metrics.update({
            "overall_score": round(updated_overall_score, 2),
            "overall_quality": updated_quality,
            "user_feedback": user_feedback
        })
        
        return updated_metrics
        
    except Exception as e:
        logging.error(f"Error updating quality with feedback: {e}")
        return previous_metrics