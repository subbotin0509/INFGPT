# app.py
from flask import Flask, render_template, request, jsonify, redirect, url_for, session
import os
import json
import time
import glob
import shutil
import secrets
import hashlib
import re
import logging
import traceback
import sqlite3
from datetime import datetime, timedelta
from functools import wraps
from werkzeug.utils import secure_filename
from utils import extract_text_from_pdf, extract_text_from_docx, highlight_text
from vectorstore import create_vectorstore, load_vectorstore
from chatbot import hybrid_answer, evaluate_answer_quality, update_answer_quality_with_feedback

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
def setup_logging():
    os.makedirs('logs', exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/app.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

setup_logging()

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB
app.config['SECRET_KEY'] = secrets.token_hex(32)  # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å–ª—É—á–∞–π–Ω—ã–π –∫–ª—é—á
ALLOWED_EXTENSIONS = {'pdf', 'docx'}

# –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫–∏, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
os.makedirs('vectorstore', exist_ok=True)
os.makedirs('cache', exist_ok=True)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
vectorstore = None
_vectorstore_loaded = False
_db_initialized = False  # –§–ª–∞–≥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ë–î

# –ü—Ä–æ—Å—Ç–∞—è —Ä–æ–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å (—Ç–æ–ª—å–∫–æ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç –ª–æ–≥–∏–Ω–∞)
TEACHER_PASSWORD = 'teacher123'

# –¢—Ä–µ–∫–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
class UsageTracker:
    def __init__(self):
        self.questions_asked = 0
        self.cache_hits = 0
        self.web_searches = 0
        self.user_likes = 0
        self.user_dislikes = 0
        
    def track_question(self, used_cache=False, used_web=False):
        self.questions_asked += 1
        if used_cache:
            self.cache_hits += 1
        if used_web:
            self.web_searches += 1
    
    def track_feedback(self, feedback):
        if feedback == "like":
            self.user_likes += 1
        elif feedback == "dislike":
            self.user_dislikes += 1
    
    def get_stats(self):
        cache_hit_rate = (self.cache_hits / self.questions_asked * 100) if self.questions_asked > 0 else 0
        total_feedback = self.user_likes + self.user_dislikes
        satisfaction_rate = (self.user_likes / total_feedback * 100) if total_feedback > 0 else 0
        
        return {
            'total_questions': self.questions_asked,
            'cache_hits': self.cache_hits,
            'cache_hit_rate': round(cache_hit_rate, 2),
            'web_searches': self.web_searches,
            'user_likes': self.user_likes,
            'user_dislikes': self.user_dislikes,
            'satisfaction_rate': round(satisfaction_rate, 2)
        }

tracker = UsageTracker()

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
class AdvancedMonitoring:
    def __init__(self):
        self.setup_monitoring_db()
    
    def setup_monitoring_db(self):
        """–°–æ–∑–¥–∞–µ—Ç –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        conn = sqlite3.connect('monitoring.db')
        cursor = conn.cursor()
        
        # –¢–∞–±–ª–∏—Ü–∞ –æ—à–∏–±–æ–∫
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS errors (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT,
                error_type TEXT,
                error_message TEXT,
                stack_trace TEXT,
                user_ip TEXT,
                endpoint TEXT,
                request_data TEXT
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ - –û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS quality_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT,
                question TEXT,
                answer_quality TEXT,
                relevance_score REAL,
                completeness_score REAL,
                confidence_score REAL,
                overall_score REAL,
                used_web BOOLEAN,
                response_time REAL,
                user_feedback TEXT,
                cache_key TEXT
            )
        ''')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ cache_key –∏ –¥–æ–±–∞–≤–ª—è–µ–º –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        try:
            cursor.execute("PRAGMA table_info(quality_metrics)")
            columns = [column[1] for column in cursor.fetchall()]
            
            if 'cache_key' not in columns:
                cursor.execute('ALTER TABLE quality_metrics ADD COLUMN cache_key TEXT')
                logging.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ cache_key –≤ —Ç–∞–±–ª–∏—Ü—É quality_metrics")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–∞–±–ª–∏—Ü—ã: {e}")
        
        conn.commit()
        conn.close()
        logging.info("Monitoring database tables created/verified")

    def log_error(self, error_type, error_message, user_ip, endpoint, request_data=None):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫ —Å –¥–µ—Ç–∞–ª—è–º–∏"""
        try:
            conn = sqlite3.connect('monitoring.db')
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO errors (timestamp, error_type, error_message, stack_trace, user_ip, endpoint, request_data)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                datetime.now().isoformat(),
                error_type,
                str(error_message),
                traceback.format_exc(),
                user_ip,
                endpoint,
                json.dumps(request_data) if request_data else None
            ))
            
            conn.commit()
            conn.close()
            logging.error(f"Error logged: {error_type} - {error_message}")
        except Exception as e:
            logging.error(f"Failed to log error: {e}")

    def log_quality_metric(self, question, answer_quality, relevance_score, completeness_score, 
                          confidence_score, overall_score, used_web, response_time, 
                          user_feedback=None, cache_key=None):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤"""
        try:
            conn = sqlite3.connect('monitoring.db')
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO quality_metrics 
                (timestamp, question, answer_quality, relevance_score, completeness_score, 
                 confidence_score, overall_score, used_web, response_time, user_feedback, cache_key)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                datetime.now().isoformat(),
                question[:500],
                answer_quality,
                relevance_score,
                completeness_score,
                confidence_score,
                overall_score,
                used_web,
                response_time,
                user_feedback,
                cache_key
            ))
            
            conn.commit()
            conn.close()
            logging.info(f"Quality metric logged for question: {question[:50]}...")
        except Exception as e:
            logging.error(f"Failed to log quality metric: {e}")

    def update_quality_feedback(self, cache_key, user_feedback):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–¥–±—ç–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            conn = sqlite3.connect('monitoring.db')
            cursor = conn.cursor()
            
            # –ù–∞—Ö–æ–¥–∏–º –∑–∞–ø–∏—Å—å –ø–æ cache_key
            cursor.execute('''
                SELECT id, overall_score 
                FROM quality_metrics 
                WHERE cache_key = ? 
                ORDER BY timestamp DESC 
                LIMIT 1
            ''', (cache_key,))
            
            result = cursor.fetchone()
            if result:
                record_id, old_score = result
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–¥–±—ç–∫–∞
                feedback_multiplier = 1.2 if user_feedback == "like" else 0.8
                new_overall_score = min(1.0, max(0.1, old_score * feedback_multiplier))
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∫–∞—á–µ—Å—Ç–≤–∞
                if new_overall_score >= 0.8:
                    new_quality = "excellent"
                elif new_overall_score >= 0.6:
                    new_quality = "good"
                elif new_overall_score >= 0.4:
                    new_quality = "fair"
                else:
                    new_quality = "poor"
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å
                cursor.execute('''
                    UPDATE quality_metrics 
                    SET answer_quality = ?, overall_score = ?, user_feedback = ?
                    WHERE id = ?
                ''', (new_quality, new_overall_score, user_feedback, record_id))
                
                conn.commit()
                logging.info(f"Updated quality for cache_key {cache_key}: {old_score:.2f} -> {new_overall_score:.2f} ({new_quality})")
                conn.close()
                return True
            else:
                logging.warning(f"No record found for cache_key: {cache_key}")
                conn.close()
                return False
                
        except Exception as e:
            logging.error(f"Failed to update quality feedback: {e}")
            return False

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
monitoring = AdvancedMonitoring()

# –î–µ–∫–æ—Ä–∞—Ç–æ—Ä—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
def login_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if session.get('role') != 'teacher':
            return jsonify({'error': '–¢—Ä–µ–±—É–µ—Ç—Å—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è'}), 401
        return f(*args, **kwargs)
    return decorated_function

def error_handler(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        try:
            return f(*args, **kwargs)
        except Exception as e:
            client_ip = get_client_ip()
            monitoring.log_error(
                type(e).__name__,
                str(e),
                client_ip,
                request.endpoint,
                request.get_json(silent=True)
            )
            logging.error(f"Error in {f.__name__}: {str(e)}")
            return jsonify({"error": "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞"}), 500
    return decorated_function

def log_action(action, user_ip, details=""):
    """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_entry = f"{timestamp} - {user_ip} - {action} - {details}\n"
    
    with open('logs/actions.log', 'a', encoding='utf-8') as f:
        f.write(log_entry)
    
    logging.info(f"Action: {action} - IP: {user_ip} - Details: {details}")

# Rate Limiting (–ø—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
class RateLimiter:
    def __init__(self):
        self.requests = {}
        
    def is_allowed(self, ip, limit=10, window=60):
        now = time.time()
        if ip not in self.requests:
            self.requests[ip] = []
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        self.requests[ip] = [req_time for req_time in self.requests[ip] if now - req_time < window]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
        if len(self.requests[ip]) < limit:
            self.requests[ip].append(now)
            return True
        return False

rate_limiter = RateLimiter()

def get_client_ip():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ IP –∫–ª–∏–µ–Ω—Ç–∞"""
    if request.headers.get('X-Forwarded-For'):
        return request.headers.get('X-Forwarded-For').split(',')[0]
    return request.remote_addr

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def validate_uploaded_file(file):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∂–∞–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞"""
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
    file.seek(0, 2)  # –ü–µ—Ä–µ–º–µ—â–∞–µ–º—Å—è –≤ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞
    file_size = file.tell()
    file.seek(0)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –≤ –Ω–∞—á–∞–ª–æ
    
    if file_size > 50 * 1024 * 1024:  # 50MB
        return False, "–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (–º–∞–∫—Å. 50MB)"
    
    # –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
    if file.filename.endswith('.pdf'):
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–≥–Ω–∞—Ç—É—Ä—ã PDF
        signature = file.read(4)
        file.seek(0)
        if signature != b'%PDF':
            return False, "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π PDF —Ñ–∞–π–ª"
    
    return True, "OK"

def sanitize_input(text):
    """–û—á–∏—Å—Ç–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –≤–≤–æ–¥–∞"""
    if not text or len(text) > 1000:
        return None
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –æ–ø–∞—Å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
    cleaned = re.sub(r'[<>{}]', '', text)
    return cleaned.strip() if cleaned.strip() else None

def get_cache_key(question):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –¥–ª—è –∫—ç—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ø—Ä–æ—Å–∞ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ñ–∞–π–ª–æ–≤"""
    files_hash = get_current_files_hash()
    base_key = f"{question}_{files_hash}"
    return hashlib.md5(base_key.encode()).hexdigest()

def get_current_files_hash():
    """–•—ç—à —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫—ç—à–∞"""
    try:
        files = sorted(os.listdir(app.config['UPLOAD_FOLDER']))
        content = "".join(files)
        return hashlib.md5(content.encode()).hexdigest()
    except:
        return "error"

def save_to_cache(question, answer_data):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç–≤–µ—Ç –≤ –∫—ç—à"""
    cache_key = get_cache_key(question)
    cache_file = f"cache/{cache_key}.json"
    
    cache_data = {
        'question': question,
        'answer_data': answer_data,
        'timestamp': time.time(),
        'cache_key': cache_key
    }
    
    try:
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        logging.info(f"Response cached with key: {cache_key}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∫—ç—à: {e}")

def load_from_cache(question):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ—Ç–≤–µ—Ç –∏–∑ –∫—ç—à–∞"""
    cache_key = get_cache_key(question)
    cache_file = f"cache/{cache_key}.json"
    
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —É—Å—Ç–∞—Ä–µ–ª –ª–∏ –∫—ç—à (1 —á–∞—Å)
                if time.time() - cache_data['timestamp'] < 3600:
                    logging.info(f"Cache hit for key: {cache_key}")
                    return cache_data
                else:
                    logging.info(f"Cache expired for key: {cache_key}")
                    os.remove(cache_file)  # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫—ç—à
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ –∫—ç—à–∞: {e}")
    return None

def format_formulas(text):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª—ã –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    replacements = [
        (r'\$\$(.*?)\$\$', r'<div class="math-formula">\1</div>'),
        (r'\$(.*?)\$', r'<span class="math-inline">\1</span>'),
        (r'\\\[(.*?)\\\]', r'<div class="math-formula">\1</div>'),
        (r'\\\((.*?)\\\)', r'<span class="math-inline">\1</span>'),
    ]
    
    for pattern, replacement in replacements:
        text = re.sub(pattern, replacement, text)
    
    return text

def get_system_stats():
    """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∏—Å—Ç–µ–º—ã"""
    try:
        files = os.listdir(app.config['UPLOAD_FOLDER'])
        files = [f for f in files if f.lower().endswith(('.pdf', '.docx'))]
        
        cache_files = glob.glob('cache/*.json')
        total_cache_size = sum(os.path.getsize(f) for f in cache_files)
        
        vectorstore_size = 0
        if os.path.exists('vectorstore'):
            for root, dirs, files in os.walk('vectorstore'):
                for file in files:
                    vectorstore_size += os.path.getsize(os.path.join(root, file))
        
        # –ü–æ–ª—É—á–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–∞
        total, used, free = shutil.disk_usage("/")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        usage_stats = tracker.get_stats()
        
        stats = {
            'total_files': len(files),
            'total_cache_entries': len(cache_files),
            'cache_size_mb': round(total_cache_size / (1024 * 1024), 2),
            'vectorstore_size_mb': round(vectorstore_size / (1024 * 1024), 2),
            'disk_usage_gb': round(used / (1024**3), 2),
            'disk_free_gb': round(free / (1024**3), 2),
            'disk_total_gb': round(total / (1024**3), 2),
            'system_uptime': get_system_uptime(),
            'usage_stats': usage_stats
        }
        
        return stats
    except Exception as e:
        logging.error(f"Error getting system stats: {e}")
        return {
            'total_files': 0,
            'total_cache_entries': 0,
            'cache_size_mb': 0,
            'vectorstore_size_mb': 0,
            'disk_usage_gb': 0,
            'disk_free_gb': 0,
            'disk_total_gb': 0,
            'system_uptime': '–ù–µ–¥–æ—Å—Ç—É–ø–Ω–æ',
            'usage_stats': tracker.get_stats()
        }

def get_system_uptime():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã"""
    try:
        if os.path.exists('/proc/uptime'):
            with open('/proc/uptime', 'r') as f:
                uptime_seconds = float(f.readline().split()[0])
                hours = int(uptime_seconds // 3600)
                minutes = int((uptime_seconds % 3600) // 60)
                return f"{hours}—á {minutes}–º"
        # –î–ª—è Windows –∏ –¥—Ä—É–≥–∏—Ö —Å–∏—Å—Ç–µ–º
        return "–ù–µ–¥–æ—Å—Ç—É–ø–Ω–æ"
    except:
        return "–ù–µ–¥–æ—Å—Ç—É–ø–Ω–æ"

def rebuild_vectorstore():
    """–ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤"""
    global vectorstore
    try:
        files = os.listdir(app.config['UPLOAD_FOLDER'])
        texts = []
        metadatas = []
        
        for filename in files:
            if filename.lower().endswith(('.pdf', '.docx')):
                filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
                
                if filename.endswith('.pdf'):
                    text = extract_text_from_pdf(filepath)
                else:
                    text = extract_text_from_docx(filepath)
                
                if text.strip():
                    texts.append(text)
                    metadatas.append({"source": filename})
        
        if texts:
            vectorstore = create_vectorstore(texts, metadatas=metadatas)
            logging.info(f"–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –ø–µ—Ä–µ—Å—Ç—Ä–æ–µ–Ω–∞ —Å {len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
            return True
        return False
    except Exception as e:
        logging.error(f"Error rebuilding vectorstore: {e}")
        return False

def get_quality_text(quality):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∫–æ–¥ –∫–∞—á–µ—Å—Ç–≤–∞ –≤ —Ç–µ–∫—Å—Ç"""
    quality_map = {
        'excellent': '–û—Ç–ª–∏—á–Ω–æ',
        'good': '–•–æ—Ä–æ—à–æ', 
        'fair': '–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ',
        'poor': '–ü–ª–æ—Ö–æ'
    }
    return quality_map.get(quality, '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')

@app.before_request
def load_existing_vectorstore():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ"""
    global vectorstore, _vectorstore_loaded
    if _vectorstore_loaded:
        return

    try:
        if os.path.exists("./vectorstore/chroma.sqlite3"):
            vectorstore = load_vectorstore()
            logging.info("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π")
            _vectorstore_loaded = True
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É: {e}")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ
@app.before_request
def initialize_database():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ"""
    global _db_initialized
    if not _db_initialized:
        try:
            monitoring.setup_monitoring_db()
            _db_initialized = True
            logging.info("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")

# Health check endpoint
@app.route('/api/health')
def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã"""
    client_ip = get_client_ip()
    log_action("health_check", client_ip)
    
    status = {
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'vectorstore_loaded': _vectorstore_loaded,
        'upload_folder_exists': os.path.exists(app.config['UPLOAD_FOLDER']),
        'cache_files_count': len(glob.glob('cache/*.json')),
        'system_uptime': get_system_uptime(),
        'db_initialized': _db_initialized
    }
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
    try:
        if vectorstore:
            test_results = vectorstore.similarity_search("test", k=1)
            status['vectorstore_working'] = True
        else:
            status['vectorstore_working'] = False
    except Exception as e:
        status['vectorstore_working'] = False
        status['status'] = 'degraded'
        status['error'] = str(e)
    
    return jsonify(status)

@app.route('/')
def index():
    client_ip = get_client_ip()
    log_action("page_view", client_ip, "index")
    return render_template('index.html')

@app.route('/teacher_login', methods=['POST'])
def teacher_login():
    client_ip = get_client_ip()
    data = request.get_json()
    password = data.get('password', '').strip()
    
    if password == TEACHER_PASSWORD:
        session['role'] = 'teacher'
        log_action("teacher_login", client_ip, "success")
        return jsonify({'success': True})
    else:
        log_action("teacher_login", client_ip, "failed")
        return jsonify({'success': False, 'error': '–ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è'})

@app.route('/logout')
def logout():
    client_ip = get_client_ip()
    session.clear()
    log_action("logout", client_ip)
    return jsonify({'success': True})

@app.route('/user_info')
def user_info():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ"""
    role = session.get('role', 'student')
    return jsonify({'role': role})

# –ê–¥–º–∏–Ω-–ø–∞–Ω–µ–ª—å
@app.route('/admin')
@login_required
def admin_dashboard():
    """–ü–∞–Ω–µ–ª—å –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞ –¥–ª—è –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–µ–π"""
    client_ip = get_client_ip()
    log_action("admin_access", client_ip)
    
    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
    files = os.listdir(app.config['UPLOAD_FOLDER'])
    files = [f for f in files if f.lower().endswith(('.pdf', '.docx'))]
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å –ü–†–ê–í–ò–õ–¨–ù–´–ú –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ñ–∞–π–ª–æ–≤
    stats = get_system_stats()
    stats['total_files'] = len(files)
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–∞—Ö
    file_info = []
    for filename in files:
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        size = os.path.getsize(filepath)
        file_info.append({
            'name': filename,
            'size_mb': round(size / (1024 * 1024), 2),
            'upload_date': time.strftime('%Y-%m-%d %H:%M', time.localtime(os.path.getctime(filepath)))
        })
    
    return render_template('admin.html', stats=stats, files=file_info)

@app.route('/admin/analytics')
@login_required
def analytics_dashboard():
    """–°—Ç—Ä–∞–Ω–∏—Ü–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
    return render_template('analytics.html')

@app.route('/admin/analytics_data')
@login_required
def analytics_data():
    """API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
    try:
        conn = sqlite3.connect('monitoring.db')
        cursor = conn.cursor()
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        cursor.execute('SELECT COUNT(*) FROM quality_metrics')
        total_questions = cursor.fetchone()[0]
        
        cursor.execute('SELECT AVG(response_time) FROM quality_metrics WHERE response_time > 0')
        avg_response_time = round(cursor.fetchone()[0] or 0, 2)
        
        cursor.execute('SELECT AVG(overall_score) FROM quality_metrics')
        avg_quality_score = round((cursor.fetchone()[0] or 0) * 100, 1)
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞
        cursor.execute('''
            SELECT answer_quality, COUNT(*) 
            FROM quality_metrics 
            GROUP BY answer_quality
        ''')
        quality_data = cursor.fetchall()
        quality_distribution = [0, 0, 0, 0]  # excellent, good, fair, poor
        for quality, count in quality_data:
            if quality == 'excellent':
                quality_distribution[0] = count
            elif quality == 'good':
                quality_distribution[1] = count
            elif quality == 'fair':
                quality_distribution[2] = count
            elif quality == 'poor':
                quality_distribution[3] = count
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤
        cursor.execute('SELECT COUNT(*) FROM quality_metrics WHERE used_web = 0')
        lecture_answers = cursor.fetchone()[0]
        cursor.execute('SELECT COUNT(*) FROM quality_metrics WHERE used_web = 1')
        web_answers = cursor.fetchone()[0]
        
        # –û—Ü–µ–Ω–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        cursor.execute('SELECT COUNT(*) FROM quality_metrics WHERE user_feedback = "like"')
        user_likes = cursor.fetchone()[0]
        cursor.execute('SELECT COUNT(*) FROM quality_metrics WHERE user_feedback = "dislike"')
        user_dislikes = cursor.fetchone()[0]
        total_feedback = user_likes + user_dislikes
        satisfaction_rate = round((user_likes / total_feedback * 100), 1) if total_feedback > 0 else 0
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–µ –≤–æ–ø—Ä–æ—Å—ã
        cursor.execute('''
            SELECT question, answer_quality, response_time, used_web, timestamp, user_feedback
            FROM quality_metrics 
            ORDER BY timestamp DESC 
            LIMIT 10
        ''')
        recent_questions = []
        for row in cursor.fetchall():
            recent_questions.append({
                'question': row[0][:100] + '...' if len(row[0]) > 100 else row[0],
                'quality': row[1],
                'quality_text': get_quality_text(row[1]),
                'response_time': round(row[2], 2),
                'source': '–ò–Ω—Ç–µ—Ä–Ω–µ—Ç' if row[3] else '–õ–µ–∫—Ü–∏–∏',
                'timestamp': row[4][11:16],  # —Ç–æ–ª—å–∫–æ –≤—Ä–µ–º—è
                'user_feedback': row[5] or '–Ω–µ—Ç –æ—Ü–µ–Ω–∫–∏'
            })
        
        # –¢–æ–ø –≤–æ–ø—Ä–æ—Å–æ–≤
        cursor.execute('''
            SELECT question, COUNT(*) as count
            FROM quality_metrics 
            GROUP BY question 
            ORDER BY count DESC 
            LIMIT 5
        ''')
        top_questions = []
        for row in cursor.fetchall():
            top_questions.append({
                'question': row[0][:50] + '...' if len(row[0]) > 50 else row[0],
                'count': row[1]
            })
        
        # –í–æ–ø—Ä–æ—Å—ã —Å –Ω–∏–∑–∫–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º
        cursor.execute('''
            SELECT question, answer_quality, overall_score
            FROM quality_metrics 
            WHERE overall_score < 0.5 
            ORDER BY overall_score ASC 
            LIMIT 5
        ''')
        low_quality_questions = []
        for row in cursor.fetchall():
            low_quality_questions.append({
                'question': row[0][:80] + '...' if len(row[0]) > 80 else row[0],
                'quality': row[1],
                'score': round(row[2], 2)
            })
        
        conn.close()
        
        return jsonify({
            'total_questions': total_questions,
            'avg_response_time': avg_response_time,
            'avg_quality_score': avg_quality_score,
            'success_rate': round((quality_distribution[0] + quality_distribution[1]) / total_questions * 100, 1) if total_questions > 0 else 0,
            'quality_distribution': quality_distribution,
            'lecture_answers': lecture_answers,
            'web_answers': web_answers,
            'user_likes': user_likes,
            'user_dislikes': user_dislikes,
            'satisfaction_rate': satisfaction_rate,
            'recent_questions': recent_questions,
            'top_questions': top_questions,
            'low_quality_questions': low_quality_questions
        })
        
    except Exception as e:
        logging.error(f"Error in analytics data: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/delete_file', methods=['POST'])
@login_required
@error_handler
def admin_delete_file():
    """–£–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª –∏–∑ —Å–∏—Å—Ç–µ–º—ã"""
    client_ip = get_client_ip()
    data = request.get_json()
    filename = data.get('filename')
    
    if not filename:
        return jsonify({'success': False, 'error': '–ù–µ —É–∫–∞–∑–∞–Ω–æ –∏–º—è —Ñ–∞–π–ª–∞'})
    
    try:
        # –ó–∞—â–∏—Ç–∞ –æ—Ç path traversal
        filename = secure_filename(filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        
        if os.path.exists(filepath):
            os.remove(filepath)
            log_action("file_deleted", client_ip, filename)
            
            # –ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
            success = rebuild_vectorstore()
            
            return jsonify({
                'success': True, 
                'message': f'–§–∞–π–ª {filename} —É–¥–∞–ª–µ–Ω',
                'vectorstore_rebuilt': success
            })
        else:
            return jsonify({'success': False, 'error': '–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω'})
            
    except Exception as e:
        log_action("file_delete_error", client_ip, f"{filename}: {str(e)}")
        monitoring.log_error("delete_file", str(e), client_ip, "/admin/delete_file", {"filename": filename})
        return jsonify({'success': False, 'error': f'–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏: {str(e)}'})

@app.route('/admin/clear_cache', methods=['POST'])
@login_required
@error_handler
def admin_clear_cache():
    """–û—á–∏—â–∞–µ—Ç –∫—ç—à –æ—Ç–≤–µ—Ç–æ–≤"""
    client_ip = get_client_ip()
    
    try:
        cache_files = glob.glob('cache/*.json')
        deleted_count = 0
        
        for cache_file in cache_files:
            os.remove(cache_file)
            deleted_count += 1
        
        log_action("cache_cleared", client_ip, f"deleted {deleted_count} files")
        
        return jsonify({
            'success': True, 
            'message': f'–ö—ç—à –æ—á–∏—â–µ–Ω. –£–¥–∞–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {deleted_count}',
            'deleted_count': deleted_count
        })
    except Exception as e:
        log_action("cache_clear_error", client_ip, str(e))
        monitoring.log_error("clear_cache", str(e), client_ip, "/admin/clear_cache")
        return jsonify({'success': False, 'error': f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –∫—ç—à–∞: {str(e)}'})

@app.route('/admin/rebuild_vectorstore', methods=['POST'])
@login_required
@error_handler
def admin_rebuild_vectorstore():
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É"""
    client_ip = get_client_ip()
    
    try:
        success = rebuild_vectorstore()
        if success:
            log_action("vectorstore_rebuilt", client_ip, "success")
            return jsonify({'success': True, 'message': '–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –ø–µ—Ä–µ—Å—Ç—Ä–æ–µ–Ω–∞'})
        else:
            log_action("vectorstore_rebuild_failed", client_ip, "no documents")
            return jsonify({'success': False, 'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ—Å—Ç—Ä–æ–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É'})
    except Exception as e:
        log_action("vectorstore_rebuild_error", client_ip, str(e))
        monitoring.log_error("rebuild_vectorstore", str(e), client_ip, "/admin/rebuild_vectorstore")
        return jsonify({'success': False, 'error': f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Å—Ç—Ä–æ–µ–Ω–∏–∏: {str(e)}'})

@app.route('/upload', methods=['POST'])
@login_required
@error_handler
def upload_file():
    client_ip = get_client_ip()
    global vectorstore, _vectorstore_loaded

    if 'file' not in request.files:
        return "–ù–µ—Ç —Ñ–∞–π–ª–∞", 400
        
    file = request.files['file']
    if file.filename == '':
        return "–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω", 400
        
    if file and allowed_file(file.filename):
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞
        is_valid, message = validate_uploaded_file(file)
        if not is_valid:
            log_action("file_upload_rejected", client_ip, f"{file.filename}: {message}")
            return message, 400

        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(filepath)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        try:
            if filename.endswith('.pdf'):
                text = extract_text_from_pdf(filepath)
            else:
                text = extract_text_from_docx(filepath)

            if not text.strip():
                os.remove(filepath)
                log_action("file_upload_empty", client_ip, filename)
                return "–§–∞–π–ª –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç", 400

            # –°–æ–∑–¥–∞—ë–º –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
            vectorstore = create_vectorstore([text], metadatas=[{"source": filename}])
            _vectorstore_loaded = True
            
            log_action("file_upload_success", client_ip, filename)

            return redirect(url_for('admin_dashboard'))

        except Exception as e:
            os.remove(filepath)
            log_action("file_upload_error", client_ip, f"{filename}: {str(e)}")
            monitoring.log_error("upload_file", str(e), client_ip, "/upload", {"filename": filename})
            return f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {str(e)}", 400

    log_action("file_upload_invalid", client_ip, file.filename)
    return "–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç", 400

@app.route('/ask', methods=['POST'])
@error_handler
def ask_question():
    client_ip = get_client_ip()
    
    # Rate limiting
    if not rate_limiter.is_allowed(client_ip, limit=10, window=60):
        log_action("rate_limit_exceeded", client_ip)
        return jsonify({"error": "–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."}), 429
    
    global vectorstore
    if not vectorstore:
        return jsonify({"error": "–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –ª–µ–∫—Ü–∏—é!"})

    data = request.get_json()
    question = data.get('question', '').strip()
    
    # –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –≤–≤–æ–¥–∞
    question = sanitize_input(question)
    if not question:
        return jsonify({"error": "–ü—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å"})

    log_action("question_asked", client_ip, f"length: {len(question)}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    cached_data = load_from_cache(question)
    if cached_data:
        tracker.track_question(used_cache=True)
        log_action("cache_hit", client_ip)
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç–∞, –±–µ–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫—ç—à–∞
        return jsonify(cached_data['answer_data'])

    try:
        start_time = time.time()
        
        # –ü–æ–ª—É—á–∞–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –æ—Ç–≤–µ—Ç (–ª–µ–∫—Ü–∏–∏ + –∏–Ω—Ç–µ—Ä–Ω–µ—Ç)
        answer, docs, used_web = hybrid_answer(question, vectorstore)
        tracker.track_question(used_web=used_web)

        response_time = time.time() - start_time
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞
        quality_metrics = evaluate_answer_quality(question, answer, docs, used_web)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º cache_key –¥–ª—è —Å–≤—è–∑–∏ —Å —Ñ–∏–¥–±—ç–∫–æ–º
        cache_key = get_cache_key(question)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        monitoring.log_quality_metric(
            question=question,
            answer_quality=quality_metrics["overall_quality"],
            relevance_score=quality_metrics["relevance_score"],
            completeness_score=quality_metrics["completeness_score"],
            confidence_score=quality_metrics["confidence_score"],
            overall_score=quality_metrics["overall_score"],
            used_web=used_web,
            response_time=response_time,
            cache_key=cache_key
        )

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ñ–æ—Ä–º—É–ª
        formatted_answer = format_formulas(answer)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
        response_data = {
            "answer": formatted_answer,
            "used_web": used_web,
            "quality_metrics": quality_metrics,
            "cache_key": cache_key  # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è —Ñ–∏–¥–±—ç–∫–∞
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –∏–∑ –ª–µ–∫—Ü–∏–π
        if not used_web and docs:
            sources = []
            for doc in docs:
                formatted_content = format_formulas(doc.page_content)
                highlighted = highlight_text(formatted_content, question)
                sources.append({
                    "content": highlighted,
                    "source": doc.metadata.get("source", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
                })
            response_data["sources"] = sources
        else:
            response_data["sources"] = []

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        save_to_cache(question, response_data)
        log_action("question_answered", client_ip, f"web_search: {used_web}, quality: {quality_metrics['overall_quality']}")

        return jsonify(response_data)

    except Exception as e:
        log_action("question_error", client_ip, str(e))
        monitoring.log_error("ask_question", str(e), client_ip, "/ask", {"question": question})
        return jsonify({"error": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}"})

@app.route('/feedback', methods=['POST'])
@error_handler
def handle_feedback():
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ª–∞–π–∫–æ–≤/–¥–∏–∑–ª–∞–π–∫–æ–≤ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
    client_ip = get_client_ip()
    data = request.get_json()
    
    cache_key = data.get('cache_key')
    feedback = data.get('feedback')  # 'like' –∏–ª–∏ 'dislike'
    
    logging.info(f"Feedback received - cache_key: {cache_key}, feedback: {feedback}")
    
    if not cache_key or feedback not in ['like', 'dislike']:
        return jsonify({'success': False, 'error': '–ù–µ–≤–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ'})
    
    try:
        # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        success = monitoring.update_quality_feedback(cache_key, feedback)
        
        if success:
            # –¢—Ä–µ–∫–∞–µ–º —Ñ–∏–¥–±—ç–∫ –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ
            tracker.track_feedback(feedback)
            log_action("user_feedback", client_ip, f"{feedback} for cache_key {cache_key}")
            
            return jsonify({
                'success': True, 
                'message': '–°–ø–∞—Å–∏–±–æ –∑–∞ –æ—Ü–µ–Ω–∫—É!'
            })
        else:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∑–∞–ø–∏—Å—å –ø–æ cache_key, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é
            logging.warning(f"Record not found for cache_key: {cache_key}, creating new feedback entry")
            
            # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –∑–∞–ø–∏—Å—å –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–∏–¥–±—ç–∫–∞
            conn = sqlite3.connect('monitoring.db')
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO quality_metrics 
                (timestamp, question, answer_quality, relevance_score, completeness_score, 
                 confidence_score, overall_score, used_web, response_time, user_feedback, cache_key)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                datetime.now().isoformat(),
                "Feedback only",
                "good" if feedback == "like" else "poor",
                0.7 if feedback == "like" else 0.3,
                0.7 if feedback == "like" else 0.3,
                0.7 if feedback == "like" else 0.3,
                0.7 if feedback == "like" else 0.3,
                False,
                0.0,
                feedback,
                cache_key
            ))
            
            conn.commit()
            conn.close()
            
            # –¢—Ä–µ–∫–∞–µ–º —Ñ–∏–¥–±—ç–∫ –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ
            tracker.track_feedback(feedback)
            log_action("user_feedback_new", client_ip, f"{feedback} for cache_key {cache_key}")
            
            return jsonify({
                'success': True, 
                'message': '–°–ø–∞—Å–∏–±–æ –∑–∞ –æ—Ü–µ–Ω–∫—É!'
            })
            
    except Exception as e:
        log_action("feedback_error", client_ip, str(e))
        monitoring.log_error("handle_feedback", str(e), client_ip, "/feedback", data)
        return jsonify({'success': False, 'error': f'–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∏–¥–±—ç–∫–∞: {str(e)}'})

@app.route('/get_files')
@error_handler
def get_files():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    try:
        files = os.listdir(app.config['UPLOAD_FOLDER'])
        files = [f for f in files if f.lower().endswith(('.pdf', '.docx'))]
        return jsonify(files)
    except Exception as e:
        logging.error(f"Error getting files: {e}")
        return jsonify([])

@app.route('/api/usage_stats')
@login_required
@error_handler
def get_usage_stats():
    """API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    return jsonify(tracker.get_stats())

if __name__ == '__main__':
    logging.info("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è INFGPT —Å —Å–∏—Å—Ç–µ–º–æ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫")
    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    try:
        monitoring.setup_monitoring_db()
        logging.info("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
    
    app.run(debug=True, host='0.0.0.0', port=5000)